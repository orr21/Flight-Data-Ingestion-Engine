{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c36968-b9f2-4bf8-b67c-8c6b187ec9b4",
   "metadata": {},
   "source": [
    "# A Simple Ingestion Engine for the Flights Dataset\n",
    "\n",
    "This project consists of an implementation of an ingestion engine skeleton for JSON files received daily with information about flights that took place on the day indicated in the file name between two airports in the United States.\n",
    "* The structure of each JSON can be consulted by opening any of the files with a text editor.\n",
    "* The meaning of each column can be found in the config.json file located in the config folder of the repository.\n",
    "\n",
    "This notebook only validates that the code of the ingestion engine with which the package was generated is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c094eb8b-70fa-4df4-9673-4abb88538aac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: loguru==0.7.1 in /Users/orr21/Documents/UCM/Master/Spark/spark-tarea-final/.venv/lib/python3.12/site-packages (0.7.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install loguru==0.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3c5f4-d0f8-48a6-9cf2-95011e916646",
   "metadata": {},
   "source": [
    "## Wheel Installation in the Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffd4cc4-888b-4f86-91f0-6e2fb2d36147",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/orr21/Documents/UCM/Master/Spark/spark-tarea-final/motor_ingesta-0.1.0-py3-none-any.whl\n",
      "Installing collected packages: motor-ingesta\n",
      "  Attempting uninstall: motor-ingesta\n",
      "    Found existing installation: motor-ingesta 0.1.0\n",
      "    Uninstalling motor-ingesta-0.1.0:\n",
      "      Successfully uninstalled motor-ingesta-0.1.0\n",
      "Successfully installed motor-ingesta-0.1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall ../motor_ingesta-0.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c2b607-b1ac-40d4-b439-f9d1775fe17d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b86cb75d3279fd2c37567cfcadc8282",
     "grade": false,
     "grade_id": "cell-e7e47b57728a3497",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Testing File Ingestion\n",
    "\n",
    "**Exercise 1 (2 points)**. The **`2023-01-01.json`** file is ingested using the complete ingestion engine. A MotorIngesta class object is created and the `ingesta_fichero` method is used, leaving the result in the `flights_df` variable. The `flujo_diario` variable contains a FlujoDiario object initialized with the previous configuration path, and it is used here only to properly read the configuration and pass it to the `motor_ingesta` object as the config argument.\n",
    "\n",
    "* This exercise required having previously completed the Python package code and generated the .whl file, and therefore, the exercise score is due to that work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a218d7f9-a274-424d-b1bd-5c2e80c8265a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34de26b267263009f0b64630904f826e",
     "grade": false,
     "grade_id": "ingesta_fichero",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/11 16:22:21 WARN Utils: Your hostname, MacBook-Pro-de-Oscar.local resolves to a loopback address: 127.0.0.1; using 192.168.1.112 instead (on interface en0)\n",
      "25/09/11 16:22:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/11 16:22:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/09/11 16:22:24 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+---------------+------+---------------+-----------+-------------+----+---------------+---------+--------+-------+--------+-------+---------+--------+-------+--------+\n",
      "|FlightDate|Reporting_Airline|OriginAirportID|Origin| OriginCityName|OriginState|DestAirportID|Dest|   DestCityName|DestState|DepDelay|DepTime|ArrDelay|ArrTime|Cancelled|Diverted|AirTime|Distance|\n",
      "+----------+-----------------+---------------+------+---------------+-----------+-------------+----+---------------+---------+--------+-------+--------+-------+---------+--------+-------+--------+\n",
      "|2023-01-01|               9E|          12884|   LAN|    Lansing, MI|         MI|        11433| DTW|    Detroit, MI|       MI|      -6|   1136|     -22|   1215|    false|   false|     21|      74|\n",
      "|2023-01-01|               DL|          13487|   MSP|Minneapolis, MN|         MN|        13495| MSY|New Orleans, LA|       LA|      -1|   1015|     -17|   1244|    false|   false|    133|    1039|\n",
      "|2023-01-01|               AS|          14747|   SEA|    Seattle, WA|         WA|        14679| SAN|  San Diego, CA|       CA|       1|   1946|       8|   2241|    false|   false|    131|    1050|\n",
      "|2023-01-01|               AA|          15412|   TYS|  Knoxville, TN|         TN|        11057| CLT|  Charlotte, NC|       NC|      -3|   1610|      10|   1735|    false|   false|     35|     177|\n",
      "|2023-01-01|               AA|          14107|   PHX|    Phoenix, AZ|         AZ|        11057| CLT|  Charlotte, NC|       NC|      -7|    553|      -7|   1147|    false|   false|    207|    1773|\n",
      "+----------+-----------------+---------------+------+---------------+-----------+-------------+----+---------------+---------+--------+-------+--------+-------+---------+--------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from motor_ingesta.motor_ingesta import MotorIngesta\n",
    "import json\n",
    "\n",
    "path_config_flujo_diario = \"../config/config.json\"       # path to the config.json file, which does not belong to the package\n",
    "path_json_primer_dia = \"../data/landing/2023-01-01.json\"          # path to the JSON file for a specific day we want to ingest, in our case 2023-01-01.json\n",
    "\n",
    "with open(path_config_flujo_diario) as f:\n",
    "    flujo_diario = json.load(f)\n",
    "motor_ingesta = MotorIngesta(flujo_diario)\n",
    "flights_df = motor_ingesta.ingesta_fichero(path_json_primer_dia)\n",
    "\n",
    "flights_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0536b275-3592-4dd5-bd99-576d0039c883",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b779d274c03920f7902ecf1ba28ef59e",
     "grade": true,
     "grade_id": "ingesta-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert(flights_df.count() == 15856)\n",
    "assert(len(flights_df.columns) == 18)\n",
    "dtypes = dict(flights_df.dtypes)\n",
    "assert(dtypes[\"Diverted\"] == \"boolean\")\n",
    "assert(dtypes[\"ArrTime\"] == \"int\")\n",
    "assert(flights_df.schema[\"Dest\"].metadata == {\"comment\": \"Destination Airport IATA code (3 letters)\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e03479e-42a6-4ee7-9f34-ebc661294c06",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3db784f6afa844cf5c8ff545533b6c4a",
     "grade": false,
     "grade_id": "cell-d3f1684d9a8578bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Testing the Function to Add Time in UTC Format\n",
    "\n",
    "**Exercise 2 (2 points)** Testing the function to add UTC time with the `flights_df` DataFrame built previously. The result is stored in the `flights_with_utc` variable. Note that this is not strictly a unit test.\n",
    "\n",
    "* This exercise required having previously completed the code for the `aniade_hora_utc` function in the Python package and generated the .whl file, and therefore, the exercise score is due to that work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c6e45d-ef89-41f0-821d-39a24de9245e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3674aeefedb405cd73689ee930980846",
     "grade": false,
     "grade_id": "flights-utc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/11 16:22:26 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+---------------+------+---------------+-----------+-------------+----+---------------+---------+--------+-------+--------+-------+---------+--------+-------+--------+-------------------+\n",
      "|FlightDate|Reporting_Airline|OriginAirportID|Origin| OriginCityName|OriginState|DestAirportID|Dest|   DestCityName|DestState|DepDelay|DepTime|ArrDelay|ArrTime|Cancelled|Diverted|AirTime|Distance|         FlightTime|\n",
      "+----------+-----------------+---------------+------+---------------+-----------+-------------+----+---------------+---------+--------+-------+--------+-------+---------+--------+-------+--------+-------------------+\n",
      "|2023-01-01|               9E|          12884|   LAN|    Lansing, MI|         MI|        11433| DTW|    Detroit, MI|       MI|      -6|   1136|     -22|   1215|    false|   false|     21|      74|2023-01-01 16:36:00|\n",
      "|2023-01-01|               DL|          13487|   MSP|Minneapolis, MN|         MN|        13495| MSY|New Orleans, LA|       LA|      -1|   1015|     -17|   1244|    false|   false|    133|    1039|2023-01-01 16:15:00|\n",
      "|2023-01-01|               AS|          14747|   SEA|    Seattle, WA|         WA|        14679| SAN|  San Diego, CA|       CA|       1|   1946|       8|   2241|    false|   false|    131|    1050|2023-01-02 03:46:00|\n",
      "|2023-01-01|               AA|          15412|   TYS|  Knoxville, TN|         TN|        11057| CLT|  Charlotte, NC|       NC|      -3|   1610|      10|   1735|    false|   false|     35|     177|2023-01-01 21:10:00|\n",
      "|2023-01-01|               AA|          14107|   PHX|    Phoenix, AZ|         AZ|        11057| CLT|  Charlotte, NC|       NC|      -7|    553|      -7|   1147|    false|   false|    207|    1773|2023-01-01 12:53:00|\n",
      "+----------+-----------------+---------------+------+---------------+-----------+-------------+----+---------------+---------+--------+-------+--------+-------+---------+--------+-------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from motor_ingesta.agregaciones import aniade_hora_utc\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Actividad Spark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "flights_with_utc = aniade_hora_utc(spark, flights_df)\n",
    "\n",
    "flights_with_utc.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d490ed43-a3f2-455c-94a9-f074448469f8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6600ce9efa93f23fcafd8a5c4571af25",
     "grade": true,
     "grade_id": "flights-utc-tests",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "assert(flights_with_utc.where(\"FlightTime is null\").count() == 266)\n",
    "types = dict(flights_with_utc.dtypes)\n",
    "assert(flights_with_utc.dtypes[18] == (\"FlightTime\", \"timestamp\"))\n",
    "\n",
    "first_row = flights_with_utc.where(\"OriginAirportID = 12884\").select(F.min(\"FlightTime\").cast(\"string\").alias(\"FlightTime\")).first()\n",
    "assert(first_row.FlightTime == \"2023-01-01 10:59:00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f241ea1-fe78-44da-825d-a9426de1dd24",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4b7d83307caa7e39620489749764e78",
     "grade": false,
     "grade_id": "cell-068b74be5a0b9aaa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Testing the Function to Add Columns with Next Flight Time, Airline, and Elapsed Time Interval\n",
    "\n",
    "**Exercise 3 (2.5 points)** The function to add intervals by airport is invoked, starting from the `flights_with_utc` variable from the previous section, storing the result returned by the function in the cached `df_with_next_flight` variable.\n",
    "\n",
    "* This exercise required having previously completed the code for the `aniade_intervalos_por_aeropuerto` function in the Python package and generated the .whl file, and therefore, the exercise score is due to that work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04f88ca-5267-4821-af63-e301012e94d4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54149f06e69c8b3d8f8f356c4af06fd9",
     "grade": false,
     "grade_id": "flights-intervalos",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+---------------+------+------------------------------+-----------+-------------+----+---------------------+---------+--------+-------+--------+-------+---------+--------+-------+--------+-------------------+-------------------+------------+---------+\n",
      "|FlightDate|Reporting_Airline|OriginAirportID|Origin|OriginCityName                |OriginState|DestAirportID|Dest|DestCityName         |DestState|DepDelay|DepTime|ArrDelay|ArrTime|Cancelled|Diverted|AirTime|Distance|FlightTime         |FlightTime_next    |Airline_next|diff_next|\n",
      "+----------+-----------------+---------------+------+------------------------------+-----------+-------------+----+---------------------+---------+--------+-------+--------+-------+---------+--------+-------+--------+-------------------+-------------------+------------+---------+\n",
      "|2023-01-01|9E               |10135          |ABE   |Allentown/Bethlehem/Easton, PA|PA         |10397        |ATL |Atlanta, GA          |GA       |1       |601    |-9      |816    |false    |false   |113    |692     |2023-01-01 11:01:00|2023-01-01 12:17:00|MQ          |4560     |\n",
      "|2023-01-01|MQ               |10135          |ABE   |Allentown/Bethlehem/Easton, PA|PA         |13930        |ORD |Chicago, IL          |IL       |-8      |717    |-17     |833    |false    |false   |114    |655     |2023-01-01 12:17:00|2023-01-01 12:52:00|OH          |2100     |\n",
      "|2023-01-01|OH               |10135          |ABE   |Allentown/Bethlehem/Easton, PA|PA         |11057        |CLT |Charlotte, NC        |NC       |-8      |752    |-39     |934    |false    |false   |78     |481     |2023-01-01 12:52:00|2023-01-01 15:21:00|G4          |8940     |\n",
      "|2023-01-01|G4               |10135          |ABE   |Allentown/Bethlehem/Easton, PA|PA         |14112        |PIE |St. Petersburg, FL   |FL       |21      |1021   |28      |1310   |false    |false   |151    |970     |2023-01-01 15:21:00|2023-01-01 16:06:00|G4          |2700     |\n",
      "|2023-01-01|G4               |10135          |ABE   |Allentown/Bethlehem/Easton, PA|PA         |14082        |PGD |Punta Gorda, FL      |FL       |6       |1106   |3       |1353   |false    |false   |150    |1018    |2023-01-01 16:06:00|2023-01-01 16:44:00|OH          |2280     |\n",
      "|2023-01-01|OH               |10135          |ABE   |Allentown/Bethlehem/Easton, PA|PA         |11057        |CLT |Charlotte, NC        |NC       |-6      |1144   |-26     |1329   |false    |false   |81     |481     |2023-01-01 16:44:00|2023-01-01 21:32:00|G4          |17280    |\n",
      "|2023-01-01|G4               |10135          |ABE   |Allentown/Bethlehem/Easton, PA|PA         |11697        |FLL |Fort Lauderdale, FL  |FL       |256     |1632   |257     |1924   |false    |false   |151    |1041    |2023-01-01 21:32:00|2023-01-01 22:24:00|G4          |3120     |\n",
      "|2023-01-01|G4               |10135          |ABE   |Allentown/Bethlehem/Easton, PA|PA         |14761        |SFB |Sanford, FL          |FL       |30      |1724   |17      |1941   |false    |false   |117    |882     |2023-01-01 22:24:00|2023-01-01 22:34:00|OH          |600      |\n",
      "|2023-01-01|OH               |10135          |ABE   |Allentown/Bethlehem/Easton, PA|PA         |11057        |CLT |Charlotte, NC        |NC       |30      |1734   |2       |1910   |false    |false   |79     |481     |2023-01-01 22:34:00|NULL               |NULL        |NULL     |\n",
      "|2023-01-01|MQ               |10136          |ABI   |Abilene, TX                   |TX         |11298        |DFW |Dallas/Fort Worth, TX|TX       |8       |1239   |-3      |1325   |false    |false   |33     |158     |2023-01-01 18:39:00|2023-01-01 21:44:00|MQ          |11100    |\n",
      "+----------+-----------------+---------------+------+------------------------------+-----------+-------------+----+---------------------+---------+--------+-------+--------+-------+---------+--------+-------+--------+-------------------+-------------------+------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from motor_ingesta.agregaciones import aniade_intervalos_por_aeropuerto\n",
    "\n",
    "df_with_next_flight = aniade_intervalos_por_aeropuerto(flights_with_utc)\n",
    "\n",
    "df_with_next_flight.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7abe87-4b35-46b8-9376-895d92330fb7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e89190d3c04bf02b372ce8df78f18be9",
     "grade": true,
     "grade_id": "flights-intervalos-test",
     "locked": true,
     "points": 2.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert(df_with_next_flight.dtypes[19] == (\"FlightTime_next\", \"timestamp\"))\n",
    "assert(df_with_next_flight.dtypes[20] == (\"Airline_next\", \"string\"))\n",
    "assert(df_with_next_flight.dtypes[21] == (\"diff_next\", \"bigint\"))\n",
    "\n",
    "first_row = df_with_next_flight.where(\"OriginAirportID = 12884\")\\\n",
    "                               .select(F.col(\"FlightTime\").cast(\"string\"), \n",
    "                                       F.col(\"FlightTime_next\").cast(\"string\"), \n",
    "                                       F.col(\"Airline_next\"),\n",
    "                                       F.col(\"diff_next\")).sort(\"FlightTime\").first()\n",
    "\n",
    "assert(first_row.FlightTime_next == \"2023-01-01 16:36:00\")\n",
    "assert(first_row.Airline_next == \"9E\")\n",
    "assert(first_row.diff_next == 20220)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f76bd5-37f3-4afc-8ffb-6a6b6cb5f4c3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b2fd2f171470ce5d27ba06300f08677",
     "grade": false,
     "grade_id": "cell-a3539980f6a4e471",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Fixing Null Values in the 3 `_next` Columns for the Last Flight of Each Airport Each Day\n",
    "\n",
    "**Exercise 4 (2.5 points)**\n",
    "\n",
    "As the daily flow logic is currently implemented, the last flight of each day will not have the FlightTime_next column populated because data from the next day is not yet available. This behavior is fixed to resolve the null values by modifying the code of the `procesa_diario` method so that, before writing the current day's data, the three `_next` columns in the data from the day before the one we are ingesting have been corrected. A simple (though not necessarily optimal) way to achieve this was:\n",
    "* Reading from the table the partition that was written the previous day, if such table and partition exist.\n",
    "* Adding to the DataFrame returned by `aniade_hora_utc` the 3 columns it lacks to have the same structure as the table, which are `FlightTime_next`, `Airline_next`, and `diff_next` (they can be in that order if the `aniade_intervalos_por_aeropuerto` function was implemented to add them in that order), but without giving them a value (with None value, converting each column to the appropriate data type so that it later fits with the existing table).\n",
    "* Unioning the DataFrame from the previous day with the one just calculated.\n",
    "* Invoking `aniade_intervalos_por_aeropuerto` passing as argument the DataFrame resulting from the union.\n",
    "\n",
    "Apart from a unit test (which is left as optional but without points), functionality is verified by invoking `procesa_diario` from the daily flow, with files from two consecutive days, and then checking what has been written to the table after ingesting the second file. It is tested with days 1 and 2 of January 2023.\n",
    "\n",
    "* This exercise required having previously completed the Python package code and generated the .whl file, and therefore, the exercise score is due to that work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29d8976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-11 16:22:30.056\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmotor_ingesta.flujo_diario\u001b[0m:\u001b[36mprocesa_diario\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1mNo se han podido leer datos del día 2022-12-31: [TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`flights` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
      "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
      "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n",
      "'UnresolvedRelation [default, flights], [], false\n",
      "\u001b[0m\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from motor_ingesta.flujo_diario import FlujoDiario\n",
    "\n",
    "ruta_config = \"../config/config.json\"\n",
    "path_json_primer_dia = \"../data/landing/2023-01-01.json\"\n",
    "\n",
    "flujo = FlujoDiario(ruta_config)\n",
    "flujo.procesa_diario(path_json_primer_dia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a8f73b-f989-4ac2-8625-34e9e3743197",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0cc3bf8f53f02529c5da731e260b8c57",
     "grade": false,
     "grade_id": "procesa-diario",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-11 16:22:31.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmotor_ingesta.flujo_diario\u001b[0m:\u001b[36mprocesa_diario\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mLeída partición del día 2023-01-01 con éxito\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "path_json_segundo_dia = \"../data/landing/2023-01-02.json\"  # path to the 2023-01-02.json file\n",
    "\n",
    "flujo.procesa_diario(path_json_segundo_dia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693fa18d-77e3-4e5a-9243-13ecc9e0d2b8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b967ae19d8e854ebe370fd146fd86f68",
     "grade": true,
     "grade_id": "tests-unitarios",
     "locked": true,
     "points": 2.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/11 16:22:38 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "vuelos = spark.read.table(\"default.flights\").sort(\"Origin\", \"FlightTime\")\n",
    "assert(vuelos.count() == 33931)\n",
    "row = vuelos.where(\"FlightDate = '2023-01-01' and  Origin = 'ABE' and DepTime = 1734\").first()\n",
    "assert(row.diff_next == 44220)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fff13aa-4672-4038-8afd-d0773546f622",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "603d2e3dc558735a0429356e1a91c287",
     "grade": false,
     "grade_id": "cell-84b4251b85b7176b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Optional Exercise\n",
    "\n",
    "**Optional Exercise (1 point)** The four unit tests found in the `test_ingesta.py` file are completed. No additional code is written in this notebook.\n",
    "\n",
    "- A final grade of up to 9.0 points can be achieved without solving this exercise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
