{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c36968-b9f2-4bf8-b67c-8c6b187ec9b4",
   "metadata": {},
   "source": [
    "# Un motor de ingesta sencillo para el dataset de vuelos\n",
    "\n",
    "### Recuerda borrar todas las líneas que dicen `raise NotImplementedError`\n",
    "\n",
    "El trabajo final consiste en la implementación guiada de un esqueleto de motor de ingesta para ficheros JSON recibidos diariamente con información de los vuelos que han tenido lugar en el día indicado en el nombre del fichero entre dos aeropuertos de los Estados Unidos.\n",
    "* La estructura de cada JSON puede consultarse abriendo con un editor de texto cualquiera de los ficheros.\n",
    "* El significado de cada una de las columnas puede consultarse en el fichero config.json que se encuentra en la carpeta config del repositorio.\n",
    "\n",
    "Vamos a probar nuestro paquete del motor de ingesta. Antes de ejecutar este notebook, es imprescindible haber completado todo el código del motor de ingesta. \n",
    "El notebook solamente valida que el código del motor de ingesta con el cual se ha generado el paquete sea correcto. Para ello, el orden en el que debemos leer, entender y completar los ficheros del repositorio es:\n",
    "\n",
    "1. Clase `MotorIngesta` en el fichero `motor_ingesta.py`. Sólo hay que completar la función `ingesta_fichero`\n",
    "2. (Opcional) Completar los tests `test_aplana` y `test_ingesta_fichero` en el fichero `test_ingesta.py`\n",
    "3. Clase `FlujoDiario` en el fichero `flujo_diario.py`. Completar primero el inicializador y luego el método `procesa_diario`. Al ir completando el código de este método, veremos que a su vez necesitamos completar las dos funciones siguientes.\n",
    "4. Función `aniade_hora_utc` en el fichero `agregaciones.py`\n",
    "5. (Opcional) El test de dicha función se llama  `test_aniade_hora_utc` en el fichero `test_ingesta.py`\n",
    "6. Función `aniade_intervalos_por_aeropuerto` en el fichero `agregaciones.py`\n",
    "7. (Opcional) El test de dicha función se llama `test_aniade_intervalos_por_aeropuerto` en el fichero `test_ingesta.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c094eb8b-70fa-4df4-9673-4abb88538aac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: loguru==0.7.1 in ./.venv/lib/python3.12/site-packages (0.7.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install loguru==0.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3c5f4-d0f8-48a6-9cf2-95011e916646",
   "metadata": {},
   "source": [
    "## Instalamos el wheel en el cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ffd4cc4-888b-4f86-91f0-6e2fb2d36147",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/orr21/Documents/UCM/Master/Spark/spark-tarea-final/motor_ingesta-0.1.0-py3-none-any.whl\n",
      "Installing collected packages: motor-ingesta\n",
      "  Attempting uninstall: motor-ingesta\n",
      "    Found existing installation: motor-ingesta 0.1.0\n",
      "    Uninstalling motor-ingesta-0.1.0:\n",
      "      Successfully uninstalled motor-ingesta-0.1.0\n",
      "Successfully installed motor-ingesta-0.1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall ../motor_ingesta-0.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c2b607-b1ac-40d4-b439-f9d1775fe17d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b86cb75d3279fd2c37567cfcadc8282",
     "grade": false,
     "grade_id": "cell-e7e47b57728a3497",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Probamos la ingesta de un fichero\n",
    "\n",
    "**Ejercicio 1 (2 puntos)**. Ingestar el fichero **`2023-01-01.json`** utilizando el motor de ingesta completo. Debe crearse el objeto de la clase MotorIngesta y utilizar el método `ingesta_fichero`, dejando el resultado en la variable `flights_df`. La variable `flujo_diario` contiene un objeto FlujoDiario inicializado con el path de configuración anterior, y lo vamos a usar ahora solo para leer adecuadamente la configuración y pasársela al objeto `motor_ingesta` como el argumento config.\n",
    "\n",
    "* Este ejercicio requiere haber completado previamente el código del paquete de Python y haber generado el fichero .whl, y por tanto, la puntuación del ejercicio se debe a ese trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a218d7f9-a274-424d-b1bd-5c2e80c8265a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34de26b267263009f0b64630904f826e",
     "grade": false,
     "grade_id": "ingesta_fichero",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/11 16:27:03 WARN Utils: Your hostname, MacBook-Pro-de-Oscar.local resolves to a loopback address: 127.0.0.1; using 192.168.1.112 instead (on interface en0)\n",
      "25/09/11 16:27:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/11 16:27:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/09/11 16:27:05 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+---------------+------+---------------+-----------+-------------+----+---------------+---------+--------+-------+--------+-------+---------+--------+-------+--------+\n",
      "|FlightDate|Reporting_Airline|OriginAirportID|Origin| OriginCityName|OriginState|DestAirportID|Dest|   DestCityName|DestState|DepDelay|DepTime|ArrDelay|ArrTime|Cancelled|Diverted|AirTime|Distance|\n",
      "+----------+-----------------+---------------+------+---------------+-----------+-------------+----+---------------+---------+--------+-------+--------+-------+---------+--------+-------+--------+\n",
      "|2023-01-01|               9E|          12884|   LAN|    Lansing, MI|         MI|        11433| DTW|    Detroit, MI|       MI|      -6|   1136|     -22|   1215|    false|   false|     21|      74|\n",
      "|2023-01-01|               DL|          13487|   MSP|Minneapolis, MN|         MN|        13495| MSY|New Orleans, LA|       LA|      -1|   1015|     -17|   1244|    false|   false|    133|    1039|\n",
      "|2023-01-01|               AS|          14747|   SEA|    Seattle, WA|         WA|        14679| SAN|  San Diego, CA|       CA|       1|   1946|       8|   2241|    false|   false|    131|    1050|\n",
      "|2023-01-01|               AA|          15412|   TYS|  Knoxville, TN|         TN|        11057| CLT|  Charlotte, NC|       NC|      -3|   1610|      10|   1735|    false|   false|     35|     177|\n",
      "|2023-01-01|               AA|          14107|   PHX|    Phoenix, AZ|         AZ|        11057| CLT|  Charlotte, NC|       NC|      -7|    553|      -7|   1147|    false|   false|    207|    1773|\n",
      "+----------+-----------------+---------------+------+---------------+-----------+-------------+----+---------------+---------+--------+-------+--------+-------+---------+--------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from motor_ingesta.motor_ingesta import MotorIngesta\n",
    "import json\n",
    "\n",
    "path_config_flujo_diario = \"../config/config.json\"       # ruta del fichero config.json, que no pertenece al paquete\n",
    "path_json_primer_dia = \"../data/landing/2023-01-01.json\"          # ruta del fichero JSON de un día concreto que queremos ingestar, en nuestro caso 2023-01-01.json\n",
    "\n",
    "with open(path_config_flujo_diario) as f:\n",
    "    flujo_diario = json.load(f)\n",
    "motor_ingesta = MotorIngesta(flujo_diario)\n",
    "flights_df = motor_ingesta.ingesta_fichero(path_json_primer_dia)\n",
    "\n",
    "flights_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0536b275-3592-4dd5-bd99-576d0039c883",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b779d274c03920f7902ecf1ba28ef59e",
     "grade": true,
     "grade_id": "ingesta-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert(flights_df.count() == 15856)\n",
    "assert(len(flights_df.columns) == 18)\n",
    "dtypes = dict(flights_df.dtypes)\n",
    "assert(dtypes[\"Diverted\"] == \"boolean\")\n",
    "assert(dtypes[\"ArrTime\"] == \"int\")\n",
    "assert(flights_df.schema[\"Dest\"].metadata == {\"comment\": \"Destination Airport IATA code (3 letters)\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e03479e-42a6-4ee7-9f34-ebc661294c06",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3db784f6afa844cf5c8ff545533b6c4a",
     "grade": false,
     "grade_id": "cell-d3f1684d9a8578bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Probamos la función de añadir la hora en formato UTC\n",
    "\n",
    "**Ejercicio 2 (2 puntos)** Probar la función de añadir hora UTC con el DF `flights_df` construido anteriormente. El resultado debe dejarse en la variable `flights_with_utc`. Recuerda que esto no es propiamente un test unitario.\n",
    "\n",
    "* Este ejercicio requiere haber completado previamente el código de la función `aniade_hora_utc` del paquete de Python y haber generado el fichero .whl, y por tanto, la puntuación del ejercicio se debe a ese trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15c6e45d-ef89-41f0-821d-39a24de9245e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3674aeefedb405cd73689ee930980846",
     "grade": false,
     "grade_id": "flights-utc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/11 16:27:13 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+---------------+------+---------------+-----------+-------------+----+---------------+---------+--------+-------+--------+-------+---------+--------+-------+--------+-------------------+\n",
      "|FlightDate|Reporting_Airline|OriginAirportID|Origin| OriginCityName|OriginState|DestAirportID|Dest|   DestCityName|DestState|DepDelay|DepTime|ArrDelay|ArrTime|Cancelled|Diverted|AirTime|Distance|         FlightTime|\n",
      "+----------+-----------------+---------------+------+---------------+-----------+-------------+----+---------------+---------+--------+-------+--------+-------+---------+--------+-------+--------+-------------------+\n",
      "|2023-01-01|               9E|          12884|   LAN|    Lansing, MI|         MI|        11433| DTW|    Detroit, MI|       MI|      -6|   1136|     -22|   1215|    false|   false|     21|      74|2023-01-01 16:36:00|\n",
      "|2023-01-01|               DL|          13487|   MSP|Minneapolis, MN|         MN|        13495| MSY|New Orleans, LA|       LA|      -1|   1015|     -17|   1244|    false|   false|    133|    1039|2023-01-01 16:15:00|\n",
      "|2023-01-01|               AS|          14747|   SEA|    Seattle, WA|         WA|        14679| SAN|  San Diego, CA|       CA|       1|   1946|       8|   2241|    false|   false|    131|    1050|2023-01-02 03:46:00|\n",
      "|2023-01-01|               AA|          15412|   TYS|  Knoxville, TN|         TN|        11057| CLT|  Charlotte, NC|       NC|      -3|   1610|      10|   1735|    false|   false|     35|     177|2023-01-01 21:10:00|\n",
      "|2023-01-01|               AA|          14107|   PHX|    Phoenix, AZ|         AZ|        11057| CLT|  Charlotte, NC|       NC|      -7|    553|      -7|   1147|    false|   false|    207|    1773|2023-01-01 12:53:00|\n",
      "+----------+-----------------+---------------+------+---------------+-----------+-------------+----+---------------+---------+--------+-------+--------+-------+---------+--------+-------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from motor_ingesta.agregaciones import aniade_hora_utc\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Actividad Spark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "flights_with_utc = aniade_hora_utc(spark, flights_df)\n",
    "\n",
    "flights_with_utc.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d490ed43-a3f2-455c-94a9-f074448469f8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6600ce9efa93f23fcafd8a5c4571af25",
     "grade": true,
     "grade_id": "flights-utc-tests",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "assert(flights_with_utc.where(\"FlightTime is null\").count() == 266)\n",
    "types = dict(flights_with_utc.dtypes)\n",
    "assert(flights_with_utc.dtypes[18] == (\"FlightTime\", \"timestamp\"))\n",
    "\n",
    "first_row = flights_with_utc.where(\"OriginAirportID = 12884\").select(F.min(\"FlightTime\").cast(\"string\").alias(\"FlightTime\")).first()\n",
    "assert(first_row.FlightTime == \"2023-01-01 10:59:00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f241ea1-fe78-44da-825d-a9426de1dd24",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d4b7d83307caa7e39620489749764e78",
     "grade": false,
     "grade_id": "cell-068b74be5a0b9aaa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Probamos la función de añadir las columnas con la hora del siguiente vuelo, su aerolínea y el intervalo de tiempo transcurrido\n",
    "\n",
    "**Ejercicio 3 (2.5 puntos)** Invocar a la función de añadir intervalos por aeropuerto, partiendo de la variable `flights_with_utc` del apartado anterior, dejando el resultado devuelto por la función en la variable `df_with_next_flight` cacheada.\n",
    "\n",
    "* Este ejercicio requiere haber completado previamente el código de la función `aniade_intervalos_por_aeropuerto` en el paquete de Python y haber generado el fichero .whl, y por tanto, la puntuación del ejercicio se debe a ese trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e04f88ca-5267-4821-af63-e301012e94d4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54149f06e69c8b3d8f8f356c4af06fd9",
     "grade": false,
     "grade_id": "flights-intervalos",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+---------------+------+------------------------------+-----------+-------------+----+---------------------+---------+--------+-------+--------+-------+---------+--------+-------+--------+-------------------+-------------------+------------+---------+\n",
      "|FlightDate|Reporting_Airline|OriginAirportID|Origin|OriginCityName                |OriginState|DestAirportID|Dest|DestCityName         |DestState|DepDelay|DepTime|ArrDelay|ArrTime|Cancelled|Diverted|AirTime|Distance|FlightTime         |FlightTime_next    |Airline_next|diff_next|\n",
      "+----------+-----------------+---------------+------+------------------------------+-----------+-------------+----+---------------------+---------+--------+-------+--------+-------+---------+--------+-------+--------+-------------------+-------------------+------------+---------+\n",
      "|2023-01-01|9E               |10135          |ABE   |Allentown/Bethlehem/Easton, PA|PA         |10397        |ATL |Atlanta, GA          |GA       |1       |601    |-9      |816    |false    |false   |113    |692     |2023-01-01 11:01:00|2023-01-01 12:17:00|MQ          |4560     |\n",
      "|2023-01-01|MQ               |10135          |ABE   |Allentown/Bethlehem/Easton, PA|PA         |13930        |ORD |Chicago, IL          |IL       |-8      |717    |-17     |833    |false    |false   |114    |655     |2023-01-01 12:17:00|2023-01-01 12:52:00|OH          |2100     |\n",
      "|2023-01-01|OH               |10135          |ABE   |Allentown/Bethlehem/Easton, PA|PA         |11057        |CLT |Charlotte, NC        |NC       |-8      |752    |-39     |934    |false    |false   |78     |481     |2023-01-01 12:52:00|2023-01-01 15:21:00|G4          |8940     |\n",
      "|2023-01-01|G4               |10135          |ABE   |Allentown/Bethlehem/Easton, PA|PA         |14112        |PIE |St. Petersburg, FL   |FL       |21      |1021   |28      |1310   |false    |false   |151    |970     |2023-01-01 15:21:00|2023-01-01 16:06:00|G4          |2700     |\n",
      "|2023-01-01|G4               |10135          |ABE   |Allentown/Bethlehem/Easton, PA|PA         |14082        |PGD |Punta Gorda, FL      |FL       |6       |1106   |3       |1353   |false    |false   |150    |1018    |2023-01-01 16:06:00|2023-01-01 16:44:00|OH          |2280     |\n",
      "|2023-01-01|OH               |10135          |ABE   |Allentown/Bethlehem/Easton, PA|PA         |11057        |CLT |Charlotte, NC        |NC       |-6      |1144   |-26     |1329   |false    |false   |81     |481     |2023-01-01 16:44:00|2023-01-01 21:32:00|G4          |17280    |\n",
      "|2023-01-01|G4               |10135          |ABE   |Allentown/Bethlehem/Easton, PA|PA         |11697        |FLL |Fort Lauderdale, FL  |FL       |256     |1632   |257     |1924   |false    |false   |151    |1041    |2023-01-01 21:32:00|2023-01-01 22:24:00|G4          |3120     |\n",
      "|2023-01-01|G4               |10135          |ABE   |Allentown/Bethlehem/Easton, PA|PA         |14761        |SFB |Sanford, FL          |FL       |30      |1724   |17      |1941   |false    |false   |117    |882     |2023-01-01 22:24:00|2023-01-01 22:34:00|OH          |600      |\n",
      "|2023-01-01|OH               |10135          |ABE   |Allentown/Bethlehem/Easton, PA|PA         |11057        |CLT |Charlotte, NC        |NC       |30      |1734   |2       |1910   |false    |false   |79     |481     |2023-01-01 22:34:00|NULL               |NULL        |NULL     |\n",
      "|2023-01-01|MQ               |10136          |ABI   |Abilene, TX                   |TX         |11298        |DFW |Dallas/Fort Worth, TX|TX       |8       |1239   |-3      |1325   |false    |false   |33     |158     |2023-01-01 18:39:00|2023-01-01 21:44:00|MQ          |11100    |\n",
      "+----------+-----------------+---------------+------+------------------------------+-----------+-------------+----+---------------------+---------+--------+-------+--------+-------+---------+--------+-------+--------+-------------------+-------------------+------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from motor_ingesta.agregaciones import aniade_intervalos_por_aeropuerto\n",
    "\n",
    "df_with_next_flight = aniade_intervalos_por_aeropuerto(flights_with_utc)\n",
    "\n",
    "df_with_next_flight.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd7abe87-4b35-46b8-9376-895d92330fb7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e89190d3c04bf02b372ce8df78f18be9",
     "grade": true,
     "grade_id": "flights-intervalos-test",
     "locked": true,
     "points": 2.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert(df_with_next_flight.dtypes[19] == (\"FlightTime_next\", \"timestamp\"))\n",
    "assert(df_with_next_flight.dtypes[20] == (\"Airline_next\", \"string\"))\n",
    "assert(df_with_next_flight.dtypes[21] == (\"diff_next\", \"bigint\"))\n",
    "\n",
    "first_row = df_with_next_flight.where(\"OriginAirportID = 12884\")\\\n",
    "                               .select(F.col(\"FlightTime\").cast(\"string\"), \n",
    "                                       F.col(\"FlightTime_next\").cast(\"string\"), \n",
    "                                       F.col(\"Airline_next\"),\n",
    "                                       F.col(\"diff_next\")).sort(\"FlightTime\").first()\n",
    "\n",
    "assert(first_row.FlightTime_next == \"2023-01-01 16:36:00\")\n",
    "assert(first_row.Airline_next == \"9E\")\n",
    "assert(first_row.diff_next == 20220)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f76bd5-37f3-4afc-8ffb-6a6b6cb5f4c3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b2fd2f171470ce5d27ba06300f08677",
     "grade": false,
     "grade_id": "cell-a3539980f6a4e471",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Corregimos que el último vuelo de cada aeropuerto y cada día tiene valor nulo en las 3 columnas `_next`\n",
    "\n",
    "**Ejercicio 4 (2.5 puntos)**\n",
    "\n",
    "Tal como está implementada la lógica del flujo diario, el último vuelo de cada día no tendrá informada la columna FlightTime_next porque no se dispone todavía de datos del día siguiente. Se pide **corregir este comportamiento** para solucionar los valores nulos, modificando el código del método `procesa_diario` de manera que, antes de escribir los datos del día actual, se hayan corregido las tres columnas `_next` en los datos del día anterior al que estamos ingestando. Una manera simple (aunque no necesariamente óptima) de conseguirlo es:\n",
    "* Leer de la tabla la partición que se escribió el día previo, si existiera dicha tabla y dicha partición.\n",
    "* Añadir al DF devuelto por `aniade_hora_utc` las 3 columnas que le faltan para tener la misma estructura que la tabla, que son `FlightTime_next`, `Airline_next` y `diff_next` (pueden ser en ese orden si la función `aniade_intervalos_por_aeropuerto` se ha implementado para añadirlas en ese orden), pero sin darles valor (con valor None, convirtiendo cada columna al tipo de dato adecuado para que después encaje con la tabla existente).\n",
    "* Unir el DF del día previo y el que acabamos de calcular\n",
    "* Invocar a `aniade_intervalos_por_aeropuerto` pasando como argumento el DF resultante de la unión.\n",
    "\n",
    "Aparte de un test unitario (que se deja como optativo pero sin puntuación), la manera de comprobar el funcionamiento será invocar a `procesa_diario` del flujo diario, con los ficheros de dos días consecutivos, y después comprobar lo que se ha escrito en la tabla tras la ingesta del segundo fichero. Lo probaremos con los días 1 y 2 de enero de 2023.\n",
    "\n",
    "* Este ejercicio requiere haber completado previamente el código del paquete de Python y haber generado el fichero .whl, y por tanto, la puntuación del ejercicio se debe a ese trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d29d8976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-11 16:27:16.753\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmotor_ingesta.flujo_diario\u001b[0m:\u001b[36mprocesa_diario\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1mNo se han podido leer datos del día 2022-12-31: [TABLE_OR_VIEW_NOT_FOUND] The table or view `default`.`flights` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
      "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
      "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n",
      "'UnresolvedRelation [default, flights], [], false\n",
      "\u001b[0m\n",
      "25/09/11 16:27:16 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from motor_ingesta.flujo_diario import FlujoDiario\n",
    "\n",
    "ruta_config = \"../config/config.json\"\n",
    "path_json_primer_dia = \"../data/landing/2023-01-01.json\"\n",
    "\n",
    "flujo = FlujoDiario(ruta_config)\n",
    "flujo.procesa_diario(path_json_primer_dia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60a8f73b-f989-4ac2-8625-34e9e3743197",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0cc3bf8f53f02529c5da731e260b8c57",
     "grade": false,
     "grade_id": "procesa-diario",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-11 16:27:18.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmotor_ingesta.flujo_diario\u001b[0m:\u001b[36mprocesa_diario\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mLeída partición del día 2023-01-01 con éxito\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "path_json_segundo_dia = \"../data/landing/2023-01-02.json\"  # path del fichero 2023-01-02.json\n",
    "\n",
    "flujo.procesa_diario(path_json_segundo_dia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "693fa18d-77e3-4e5a-9243-13ecc9e0d2b8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b967ae19d8e854ebe370fd146fd86f68",
     "grade": true,
     "grade_id": "tests-unitarios",
     "locked": true,
     "points": 2.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vuelos = spark.read.table(\"default.flights\").sort(\"Origin\", \"FlightTime\")\n",
    "assert(vuelos.count() == 33931)\n",
    "row = vuelos.where(\"FlightDate = '2023-01-01' and  Origin = 'ABE' and DepTime = 1734\").first()\n",
    "assert(row.diff_next == 44220)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fff13aa-4672-4038-8afd-d0773546f622",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "603d2e3dc558735a0429356e1a91c287",
     "grade": false,
     "grade_id": "cell-84b4251b85b7176b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Ejercicio opcional\n",
    "\n",
    "**Ejercicio opcional (1 punto)** Completa los cuatro tests unitarios que encontrarás en el fichero `test_ingesta.py`. No tienes que escribir más código en este notebook.\n",
    "\n",
    "- Se podrá optar a una calificación final de hasta 9.0 puntos sin resolver este ejercicio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
